è¿™æ˜¯ä¸€ä¸ªä¸ºæ‚¨å‡†å¤‡çš„ä¸­è‹±åŒè¯­ç‰ˆ `README.md`ã€‚æ‚¨å¯ä»¥ç›´æ¥å¤åˆ¶ä¿å­˜ã€‚

-----

# ğŸ¦… å®è§‚æ”¿ç­–å¥—åˆ©æƒ…æŠ¥ç³»ç»Ÿ | Macro Policy Arbitrage Intelligence System

**ğŸ‡¨ğŸ‡³ ä¸­æ–‡**
è¿™æ˜¯ä¸€ä¸ªåŸºäº **LLM Agent** æ¶æ„çš„è‡ªåŠ¨åŒ–æƒ…æŠ¥åˆ†æç³»ç»Ÿã€‚å®ƒèƒ½å¤Ÿè‡ªåŠ¨æŠ“å–äº’è”ç½‘æ–°é—»ï¼Œåˆ©ç”¨ AI è¿›è¡Œä¸¤é˜¶æ®µçš„â€œç­›é€‰-åˆ†æâ€å¤„ç†ï¼Œä»æµ·é‡ä¿¡æ¯ä¸­ç²¾å‡†æ•æ‰æ¶‰åŠé¡¶å±‚è®¾è®¡å˜åŠ¨ã€é˜¶å±‚è´¢å¯Œè½¬ç§»å’Œè¡Œä¸šå‡†å…¥é—¨æ§›å˜åŒ–çš„å®è§‚ä¿¡å·ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚

**ğŸ‡ºğŸ‡¸ English**
This is an automated intelligence analysis system based on **LLM Agent** architecture. It automatically scrapes internet news and utilizes AI for a two-stage "Selection-Analysis" process. It precisely captures macro signals regarding top-level design changes, wealth transfer between social classes, and industry entry barrier shifts from massive amounts of information, generating structured analysis reports.

-----

## âœ¨ æ ¸å¿ƒåŠŸèƒ½ (Core Features)

### 1\. ğŸ•·ï¸ è‡ªåŠ¨æ•°æ®é‡‡é›† (Auto-Crawler)

  * **Custom Sources**: æ”¯æŒè‡ªå®šä¹‰æ–°é—»æº URLã€‚ (Supports custom news source URLs.)
  * **Smart Deduplication**: æ™ºèƒ½å»é‡ï¼Œé˜²æ­¢é‡å¤æŠ“å–ã€‚ (Intelligent deduplication to prevent redundant scraping.)
  * **Universal Parsing**: åŸºäºå¯å‘å¼è§„åˆ™çš„é€šç”¨ç½‘é¡µè§£æã€‚ (Heuristic-based universal web page parsing.)

### 2\. ğŸ§  åŒé˜¶æ®µ AI å¤„ç†æµæ°´çº¿ (Dual-Stage AI Pipeline)

  * **Stage 1: å®è§‚çº¢åˆ©ç‹™å‡»æ‰‹ (The Macro Bonus Sniper)**
      * ä½¿ç”¨è½»é‡çº§æ¨¡å‹ (`Flash-Lite`) å¿«é€Ÿæ‰«ææ‰¹é‡æ–‡ç« æ ‡é¢˜ã€‚ (Uses lightweight `Flash-Lite` model to rapidly scan batches of article titles.)
      * æ ¹æ®â€œç¨æ”¶/ç¤¾ä¿å˜åŠ¨â€ã€â€œé€ å¯Œ/è¿”è´«ç°è±¡â€ã€â€œè¡Œä¸šå‡†å…¥å£å’â€ä¸‰å¤§ç¡¬æŒ‡æ ‡ï¼Œä»æµ·é‡èµ„è®¯ä¸­ç­›é€‰å‡º **Top 5** æœ€å…·ä»·å€¼çš„ä¿¡å·ã€‚ (Filters the **Top 5** most valuable signals based on three hard criteria: "Tax/Social Security Changes", "Wealth Creation/Poverty Return", and "Industry Entry Barriers".)
  * **Stage 2: æ”¿ç­–å¥—åˆ©åˆ†æå¸ˆ (The Policy Arbitrage Analyst)**
      * ä½¿ç”¨é«˜æ€§èƒ½æ¨¡å‹ (`Flash`) å¯¹ç­›é€‰å‡ºçš„æ–‡ç« è¿›è¡Œæ·±åº¦ç ”è¯»ã€‚ (Uses high-performance `Flash` model for deep reading of selected articles.)
      * **Structured Output**: è‡ªåŠ¨æå–â€œçŸ›ç›¾ç‚¹â€ã€â€œæ”¿ç­–æ¸©å·®â€ã€â€œè´Ÿé¢æ¸…å•â€ã€â€œå®ä½“ä¿¡æ¯â€åŠâ€œä¸€å¥è¯ç»“è®ºâ€ã€‚ (Automatically extracts "Contradictions", "Policy Temperature Gaps", "Negative Lists", "Entity Info", and a "One-sentence Conclusion".)

### 3\. ğŸ“Š å¯è§†åŒ–æƒ…æŠ¥çœ‹æ¿ (Intelligence Dashboard)

  * **Interactive UI**: åŸºäº **Streamlit** çš„äº¤äº’å¼å‰ç«¯ã€‚ (Interactive frontend based on **Streamlit**.)
  * **Timeline View**: æ”¯æŒæŒ‰æ—¶é—´è½´æŸ¥çœ‹æƒ…æŠ¥æµã€‚ (View intelligence streams chronologically.)
  * **Developer Dashboard**: æ”¯æŒåœ¨çº¿å®æ—¶è°ƒæ•´ LLM çš„ Promptï¼ˆæç¤ºè¯ï¼‰ï¼Œæ— éœ€é‡å¯æœåŠ¡ã€‚ (Supports real-time online adjustment of LLM Prompts without restarting the service.)

### 4\. ğŸ›¡ï¸ ä¼ä¸šçº§ç¨³å®šæ€§ (Enterprise-Grade Stability)

  * **JSON Schema**: æ”¯æŒ **Structured Output**ï¼Œç¡®ä¿ LLM è¾“å‡ºæ ¼å¼ 100% ç¨³å®šï¼Œæœç»è§£æé”™è¯¯ã€‚ (Supports **Structured Output** to ensure 100% stable LLM output format and eliminate parsing errors.)
  * **Resilience**: å…·å¤‡æ–­ç‚¹ç»­ä¼ å’Œç§¯å‹ä»»åŠ¡è‡ªåŠ¨æ¸…ç©ºæœºåˆ¶ã€‚ (Features resume-from-break capability and automatic backlog clearing mechanisms.)

-----

## ğŸ› ï¸ æŠ€æœ¯æ ˆ (Tech Stack)

  * **Language**: Python 3.10+
  * **LLM API**: Google Gemini (Supports `gemini-2.5-flash` series)
  * **Web Framework**: Streamlit
  * **Database**: SQLite (SQLAlchemy ORM)
  * **Scheduler**: APScheduler
  * **Crawler**: Requests + BeautifulSoup4

-----

## ğŸš€ å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1\. å…‹éš†é¡¹ç›® (Clone Repository)

```bash
git clone <your-repo-url>
cd news-intelligence-agent
```

### 2\. å®‰è£…ä¾èµ– (Install Dependencies)

å»ºè®®ä½¿ç”¨ Python è™šæ‹Ÿç¯å¢ƒ (Recommended to use Python virtual environment):

```bash
python -m venv venv
source venv/bin/activate  # Mac/Linux
# venv\Scripts\activate   # Windows

pip install -r requirements.txt
```

### 3\. é…ç½®ç¯å¢ƒå˜é‡ (Configure Environment)

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºä¸€ä¸ª `.env` æ–‡ä»¶ï¼Œå¹¶å¡«å…¥ä½ çš„ API Key (Create a `.env` file in the root directory and add your API Key):

```ini
# .env file content
GEMINI_API_KEY=your_google_api_key_here

# Optional Config / å¯é€‰é…ç½®
LLM_PROVIDER=gemini
# Keywords filter / å…³é”®è¯è¿‡æ»¤
HIGH_VALUE_KEYWORDS=AI,Policy,Economy,Reform
```

### 4\. è¿è¡Œç³»ç»Ÿ (Run System)

#### æ–¹å¼ Aï¼šå¯åŠ¨å¯è§†åŒ–çœ‹æ¿ (Option A: Launch Dashboard) - *Recommended*

è¿™æ˜¯æœ€ç›´è§‚çš„ä½¿ç”¨æ–¹å¼ï¼Œé›†æˆäº†æ‰‹åŠ¨è§¦å‘æŠ“å–å’Œæ•°æ®æŸ¥çœ‹åŠŸèƒ½ã€‚
(This is the most intuitive way, integrating manual crawl triggering and data viewing.)

```bash
streamlit run news/src/app.py
```

  * Open browser at `http://localhost:8501`.
  * Add news source URLs in the sidebar (e.g., `https://news.ycombinator.com`).
  * Click **"ğŸš€ Fetch New Data"** to start.

#### æ–¹å¼ Bï¼šåå°é™é»˜è¿è¡Œ (Option B: Headless Background Task)

å¦‚æœä½ å¸Œæœ›è®©å®ƒåœ¨æœåŠ¡å™¨åå°æ¯å°æ—¶è‡ªåŠ¨è·‘ä¸€æ¬¡ (If you want it to run automatically every hour on a server):

```bash
python news/main.py --loop
```

-----

## ğŸ“‚ é¡¹ç›®ç»“æ„ (Project Structure)

```text
news-intelligence-agent/
â”œâ”€â”€ news/
â”‚   â”œâ”€â”€ data/               # SQLite Database file / æ•°æ®åº“æ–‡ä»¶
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app.py          # Streamlit Frontend / å‰ç«¯å…¥å£
â”‚   â”‚   â”œâ”€â”€ config.py       # Configuration / é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ crawler.py      # Crawler Logic / çˆ¬è™«é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ database.py     # DB Models / æ•°æ®åº“æ¨¡å‹
â”‚   â”‚   â””â”€â”€ processor.py    # LLM Core (Selection & Analysis) / æ™ºèƒ½æ ¸å¿ƒ
â”‚   â”œâ”€â”€ main.py             # Background Scheduler / åå°è°ƒåº¦å…¥å£
â”‚   â”œâ”€â”€ list_models.py      # Utility: List Gemini Models / å·¥å…·è„šæœ¬
â”‚   â””â”€â”€ requirements.txt    # Dependencies / ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ .env                    # Env Vars (DO NOT COMMIT) / ç¯å¢ƒå˜é‡
â”œâ”€â”€ .gitignore              # Git Ignore
â””â”€â”€ README.md               # Documentation
```

-----

## âš™ï¸ é«˜çº§é…ç½® (Developer Dashboard)

åœ¨ Streamlit ç•Œé¢çš„ **"Developer Dashboard"** æ ‡ç­¾é¡µä¸­ (In the **"Developer Dashboard"** tab of the Streamlit interface):

1.  **Database Overview**: å®æ—¶ç›‘æ§æ–‡ç« æ€»æ•°ã€å·²å¤„ç†æ•°é‡å’Œé«˜ä»·å€¼æ–‡ç« æ•°é‡ã€‚ (Real-time monitoring of total, processed, and high-value articles.)
2.  **Prompt Engineering**:
      * åŠ¨æ€è°ƒæ•´â€œç‹™å‡»æ‰‹â€çš„ç­›é€‰æ ‡å‡†ã€‚ (Dynamically adjust "Sniper" selection criteria.)
      * ä¿®æ”¹â€œåˆ†æå¸ˆâ€çš„è¾“å‡ºç»´åº¦ã€‚ (Modify "Analyst" output dimensions.)
      * *Note: No need to modify JSON format instructions; the system enforces this via Schema.* (*æ³¨æ„ï¼šæ— éœ€ä¿®æ”¹ JSON æ ¼å¼è¯´æ˜ï¼Œç³»ç»Ÿåº•å±‚å·²é€šè¿‡ Schema å¼ºåˆ¶çº¦æŸã€‚*)
3.  **Data Management**: æŒ‰ ID åˆ é™¤ç‰¹å®šçš„æ–‡ç« ã€‚ (Delete specific articles by ID.)

-----

## âš ï¸ æ³¨æ„äº‹é¡¹ (Notes)

1.  **API Costs**: ç³»ç»Ÿä¼šæ¶ˆè€— Tokenã€‚è™½ç„¶ä½¿ç”¨äº† Flash-Lite è¿›è¡Œåˆç­›ä»¥èŠ‚çœæˆæœ¬ï¼Œä½†åœ¨å¤§é‡æŠ“å–æ—¶è¯·ç•™æ„ API ç”¨é‡ã€‚ (The system consumes Tokens. While Flash-Lite is used for initial screening to save costs, please monitor API usage during heavy scraping.)
2.  **Network**: è¯·ç¡®ä¿ä½ çš„è¿è¡Œç¯å¢ƒå¯ä»¥è¿æ¥åˆ° Google Gemini API æœåŠ¡ã€‚ (Ensure your runtime environment can connect to Google Gemini API services.)
3.  **Crawler Etiquette**: è¯·å‹¿å¯¹ç›®æ ‡ç½‘ç«™è¿›è¡Œè¿‡é«˜é¢‘ç‡çš„æŠ“å–ï¼Œä»¥å…è§¦å‘åçˆ¬æœºåˆ¶ã€‚ (Do not scrape target websites at excessively high frequencies to avoid triggering anti-bot mechanisms.)

-----

**License**: MIT
