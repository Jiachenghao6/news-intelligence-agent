import logging
import json
from sqlalchemy.orm import Session
from .database import Article, get_db, SessionLocal, get_setting
from .config import config
import google.generativeai as genai
import typing_extensions as typing

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Schema Definitions
class ArticleSelection(typing.TypedDict):
    id: int
    Signal: str
    Actionability: str
    Prediction: str

class PolicyAnalysis(typing.TypedDict):
    contradictions: str
    temperature_diff: str
    negative_list: str
    entities: str
    conclusion: str

class Processor:
    def __init__(self):
        self.db: Session = SessionLocal()
        self.setup_llm()

    def setup_llm(self):
        if config.LLM_PROVIDER == "gemini":
            genai.configure(api_key=config.GEMINI_API_KEY)

    def select_high_value_articles(self, articles: list[Article]):
        """
        Stage 2: Selection
        Sends a batch of titles to the LLM to select high-value ones using the "Macro Bonus Sniper" persona.
        """
        if not articles:
            return []

        # Prepare the list for the prompt
        articles_list = "\n".join([f"ID {a.id}: {a.title}" for a in articles])
        
        # Default prompt (Macro Bonus Sniper) - Purified
        default_prompt = """
        Role: å®è§‚çº¢åˆ©ç‹™å‡»æ‰‹
        Context: åªæœ‰èƒ½æ”¹å˜ç¤¾ä¼šèµ„æºåˆ†é…è§„åˆ™çš„æ–°é—»æ‰å€¼å¾—å…³æ³¨
        Criteria: 
        1. æ˜¯å¦æ¶‰åŠ[ç¨æ”¶/ç¤¾ä¿/æˆ·ç±]ç­‰é¡¶å±‚è®¾è®¡å˜åŠ¨ï¼Ÿ(æ”¿ç­–çº¢åˆ©/é»‘å¤©é¹…) 
        2. æ˜¯å¦å‡ºç°è·¨é˜¶å±‚çš„[é€ å¯Œ/è¿”è´«]ç°è±¡ï¼Ÿ(é£å£é¢„è­¦) 
        3. æ˜¯å¦æ”¹å˜äº†[ç‰¹å®šè¡Œä¸š]çš„å‡†å…¥é—¨æ§›ï¼Ÿ(ç«äº‰å£å’) 
        
        Task:
        Review the following articles. Select the TOP 5 most impactful articles based on the criteria.
        Rank them from 1 (most impactful) to 5.
        
        Articles:
        {articles_list}
        """
        
        # Load from DB or use default
        prompt_template = get_setting("prompt_selection", default_prompt)
        
        # If the user edited the prompt, they might have removed the placeholder. 
        # We need to ensure {articles_list} is in there or append it.
        if "{articles_list}" in prompt_template:
            prompt = prompt_template.format(articles_list=articles_list)
        else:
            prompt = prompt_template + "\n\nArticles:\n" + articles_list

        try:
            model = genai.GenerativeModel(config.MODEL_SELECTION)
            
            # Use response_schema for hard constraint
            response = model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=list[ArticleSelection]
                )
            )
            
            text = response.text.strip()
            # Clean up potential markdown code blocks (though less likely with schema)
            if text.startswith("```"):
                text = text.split("\n", 1)[1]
                if text.endswith("```"):
                    text = text.rsplit("\n", 1)[0]
            
            selection_results = json.loads(text)
            logger.info(f"LLM Selection Results: {len(selection_results)} items")
            
            # Sort by order in list (assuming LLM returned ranked list) and take top 5
            top_results = selection_results[:5]
            top_ids = {item['id'] for item in top_results}
            
            # Map results to a dict for easy access
            results_map = {item['id']: item for item in top_results}
            
            selected_articles = []
            # Update DB
            for article in articles:
                article.is_processed = True
                if article.id in top_ids:
                    article.is_high_value = True
                    # Prepend the selection insights to the report (or placeholder)
                    meta = results_map[article.id]
                    article.analysis_report = f"""
**ğŸ¯ ç‹™å‡»æ‰‹ç®€æŠ¥**
- **ä¿¡å·**: {meta.get('Signal')}
- **å¯æ“ä½œæ€§**: {meta.get('Actionability')}
- **é¢„æµ‹**: {meta.get('Prediction')}
---
"""
                    selected_articles.append(article)
                else:
                    article.is_high_value = False
            self.db.commit()
            
            return selected_articles
            
        except Exception as e:
            logger.error(f"Error in batch selection: {e}")
            return []

    def analyze_article(self, article: Article):
        """
        Stage 3: Analysis
        Uses stronger LLM to analyze the article using "Policy Arbitrage Analyst" persona.
        """
        # Default prompt (Policy Arbitrage Analyst) - Purified
        default_prompt = """
        Role: å†·é…·çš„æ”¿ç­–å¥—åˆ©åˆ†æå¸ˆ
        Task: Analyze the text.
        
        Content:
        {content} 
        
        Requirements:
        1. ã€çŸ›ç›¾ç‚¹ã€‘(contradictions): æå–æ–‡ä¸­â€œæ—¢è¦...åˆè¦...â€çš„å†…å®¹ï¼Œå¹¶åˆ¤æ–­å“ªä¸€ä¸ªæ˜¯å½“å‰çš„çœŸå®KPIï¼ˆæ’åœ¨åé¢æˆ–æœ‰é‡åŒ–æŒ‡æ ‡çš„ï¼‰ã€‚
        2. ã€æ¸©å·®ã€‘(temperature_diff): å¯¹æ¯”è¯¥è¡Œä¸šå»å¹´çš„å¸¸è§„è¡¨è¿°ï¼Œæå–å˜åŒ–çš„å½¢å®¹è¯ï¼ˆå¦‚ä»â€œå¤§åŠ›å‘å±•â€å˜ä¸ºâ€œè§„èŒƒæœ‰åºâ€ï¼‰ã€‚
        3. ã€è´Ÿé¢æ¸…å•ã€‘(negative_list): æå–æ‰€æœ‰â€œä¸¥ç¦â€ã€â€œä¸å¾—â€ã€â€œæ¸…ç†â€åé¢çš„å…·ä½“è¡Œä¸ºã€‚
        4. ã€å®ä½“ä¿¡æ¯ã€‘(entities): æå–æ–‡ä¸­æ‰€æœ‰çš„é‡‘é¢ã€æ—¥æœŸã€è´Ÿè´£éƒ¨é—¨ã€‚
        5. ã€ä¸€å¥è¯ç»“è®ºã€‘(conclusion): è¿™æ–‡ä»¶æ˜¯å‘é’±çš„ï¼ˆçº¢åˆ©ï¼‰ï¼Œè¿˜æ˜¯æ”¶ç½‘çš„ï¼ˆæ•´é¡¿ï¼‰ï¼Ÿ
        """
        
        prompt_template = get_setting("prompt_analysis", default_prompt)
        
        content_snippet = article.content[:8000]
        if "{content}" in prompt_template:
            prompt = prompt_template.format(content=content_snippet)
        else:
            prompt = prompt_template + "\n\nContent:\n" + content_snippet

        try:
            model = genai.GenerativeModel(config.MODEL_ANALYSIS)
            
            # Use response_schema for hard constraint
            response = model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    response_mime_type="application/json",
                    response_schema=PolicyAnalysis
                )
            )
            
            text = response.text.strip()
            # Clean up potential markdown code blocks
            if text.startswith("```"):
                text = text.split("\n", 1)[1]
                if text.endswith("```"):
                    text = text.rsplit("\n", 1)[0]
            
            try:
                data = json.loads(text)
                formatted_report = f"""
### ğŸ•µï¸ æ”¿ç­–å¥—åˆ©åˆ†æ
- **âš–ï¸ çŸ›ç›¾ç‚¹**: {data.get('contradictions')}
- **ğŸŒ¡ï¸ æ¸©å·®**: {data.get('temperature_diff')}
- **ğŸš« è´Ÿé¢æ¸…å•**: {data.get('negative_list')}
- **ğŸ›ï¸ å®ä½“ä¿¡æ¯**: {data.get('entities')}
- **ğŸ’¡ ç»“è®º**: **{data.get('conclusion')}**
"""
            except json.JSONDecodeError:
                logger.error(f"Failed to parse JSON from analysis: {text}")
                formatted_report = f"\n**Analysis Raw Output**:\n{text}"

            # Append to the existing report (which has the sniper brief)
            if article.analysis_report:
                article.analysis_report += "\n" + formatted_report
            else:
                article.analysis_report = formatted_report
                
            self.db.commit()
            logger.info(f"Analyzed article: {article.title}")
        except Exception as e:
            logger.error(f"Error analyzing article {article.id}: {e}")

    def process_pending_articles(self):
        """
        Main loop to process unprocessed articles in batches.
        """
        while True:
            # Get next batch of unprocessed articles
            # Limit to 20 at a time to fit in context window
            articles = self.db.query(Article).filter(Article.is_processed == False).limit(20).all()
            
            if not articles:
                logger.info("No more pending articles.")
                break

            logger.info(f"Processing batch of {len(articles)} articles...")
            
            # 1. Batch Selection
            high_value_articles = self.select_high_value_articles(articles)
            
            # 2. Individual Analysis
            for article in high_value_articles:
                self.analyze_article(article)

    def close(self):
        self.db.close()

if __name__ == "__main__":
    processor = Processor()
    processor.process_pending_articles()
    processor.close()
